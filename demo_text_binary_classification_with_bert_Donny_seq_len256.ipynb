{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo-text-binary-classification-with-bert-Donny.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Donnychan1606/bert_IMDb/blob/master/demo_text_binary_classification_with_bert_Donny_seq_len256.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBbAMqZ6IZuw",
        "colab_type": "text"
      },
      "source": [
        "base code borrowed from [this Google Colab Notebook](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).\n",
        "\n",
        "Refactored by [Shuyi Wang](https://www.linkedin.com/in/shuyi-wang-b3955026/)\n",
        "\n",
        "Please refer to [this Medium Article](https://medium.com/@wshuyi/how-to-do-text-binary-classification-with-bert-f1348a25d905) for detailed information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "trusted": true,
        "outputId": "4726c150-20c2-410b-d8ed-3fbe42035d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pickle\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZM71PjIOF_I",
        "colab": {}
      },
      "source": [
        "def pretty_print(result):\n",
        "    df = pd.DataFrame([result]).T\n",
        "    df.columns = [\"values\"]\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZCYOL8HbIZu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer_from_hub_module(bert_model_hub):\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(bert_model_hub)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def make_features(dataset, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN):\n",
        "    input_example = dataset.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "    features = bert.run_classifier.convert_examples_to_features(input_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    return features\n",
        "\n",
        "def create_model(bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      bert_model_hub,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n",
        "\n",
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(bert_model_hub, num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n",
        "\n",
        "def estimator_builder(bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS, label_list, LEARNING_RATE, num_train_steps, num_warmup_steps, BATCH_SIZE):\n",
        "\n",
        "    # Specify outpit directory and number of checkpoint steps to save\n",
        "    run_config = tf.estimator.RunConfig(\n",
        "        model_dir=OUTPUT_DIR,\n",
        "        save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "    model_fn = model_fn_builder(\n",
        "      bert_model_hub = bert_model_hub,\n",
        "      num_labels=len(label_list),\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "    return estimator, model_fn, run_config\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def run_on_dfs(train, test, DATA_COLUMN, LABEL_COLUMN, \n",
        "               MAX_SEQ_LENGTH = 256,\n",
        "              BATCH_SIZE = 32,\n",
        "              LEARNING_RATE = 2e-5,\n",
        "              NUM_TRAIN_EPOCHS = 3.0,\n",
        "              WARMUP_PROPORTION = 0.1,\n",
        "              SAVE_SUMMARY_STEPS = 100,\n",
        "               SAVE_CHECKPOINTS_STEPS = 10000,\n",
        "              bert_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"):\n",
        "\n",
        "    label_list = train[LABEL_COLUMN].unique().tolist()\n",
        "    \n",
        "    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n",
        "\n",
        "    train_features = make_features(train, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n",
        "    test_features = make_features(test, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n",
        "\n",
        "    num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "    estimator, model_fn, run_config = estimator_builder(\n",
        "                                  bert_model_hub, \n",
        "                                  OUTPUT_DIR, \n",
        "                                  SAVE_SUMMARY_STEPS, \n",
        "                                  SAVE_CHECKPOINTS_STEPS, \n",
        "                                  label_list, \n",
        "                                  LEARNING_RATE, \n",
        "                                  num_train_steps, \n",
        "                                  num_warmup_steps, \n",
        "                                  BATCH_SIZE)\n",
        "\n",
        "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "        features=train_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "\n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "        features=test_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "\n",
        "    result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    return result_dict, estimator\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wpUHTA8LIZu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qmP8MBvjIZvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_DIR = 'output'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV3ExI2JJMhd",
        "colab_type": "text"
      },
      "source": [
        "----- you just need to focus from here ------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSUjVoFtJVEO",
        "colab_type": "text"
      },
      "source": [
        "## Get your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxQ6CestJXZK",
        "colab_type": "code",
        "outputId": "ecea4bb2-c782-458d-96ec-c432dba959cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Donnychan1606/bert_IMDb/master/test.csv\n",
        "!wget https://raw.githubusercontent.com/Donnychan1606/bert_IMDb/master/train.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-30 04:09:14--  https://raw.githubusercontent.com/Donnychan1606/bert_IMDb/master/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12909082 (12M) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "\rtest.csv              0%[                    ]       0  --.-KB/s               \rtest.csv            100%[===================>]  12.31M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-05-30 04:09:15 (98.3 MB/s) - ‘test.csv’ saved [12909082/12909082]\n",
            "\n",
            "--2019-05-30 04:09:16--  https://raw.githubusercontent.com/Donnychan1606/bert_IMDb/master/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43221547 (41M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  41.22M   177MB/s    in 0.2s    \n",
            "\n",
            "2019-05-30 04:09:17 (177 MB/s) - ‘train.csv’ saved [43221547/43221547]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XFBwDlmnIZvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Tju0c4dqIZvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.sample(len(train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8i4a5xUFfe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = test.sample(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9BAxyKhKirc",
        "colab_type": "code",
        "outputId": "38754346-08a9-4178-faa9-192ff74447f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4134</th>\n",
              "      <td>I remember back when I was little when I was a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9090</th>\n",
              "      <td>I've waited a long time to see DR TARR'S TORTU...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27268</th>\n",
              "      <td>I enjoyed the beautiful scenery in this movie ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16613</th>\n",
              "      <td>It's a good show, and I find it funny. Finally...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19624</th>\n",
              "      <td>The penultimate episode of Star Trek's third s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review  label\n",
              "4134   I remember back when I was little when I was a...      0\n",
              "9090   I've waited a long time to see DR TARR'S TORTU...      0\n",
              "27268  I enjoyed the beautiful scenery in this movie ...      0\n",
              "16613  It's a good show, and I find it funny. Finally...      1\n",
              "19624  The penultimate episode of Star Trek's third s...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDh-VkaxIcJq",
        "colab_type": "code",
        "outputId": "70d335f5-be46-45d0-ba85-546cb7eb7634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape\n",
        "#test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32500, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXUuX8u8Itdc",
        "colab_type": "code",
        "outputId": "bca240bf-f511-4d97-a28d-2dfd8d1b1f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM8M7k0gKk5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myparam = {\n",
        "        \"DATA_COLUMN\": \"review\",\n",
        "        \"LABEL_COLUMN\": \"label\",\n",
        "        \"LEARNING_RATE\": 2e-5,\n",
        "        \"NUM_TRAIN_EPOCHS\":3\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dg2apeXpMqG-",
        "outputId": "e2f097e8-ffe2-46e8-bdfe-cba36d0a8135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5681
        }
      },
      "source": [
        "result, estimator = run_on_dfs(train, test, **myparam)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:21.059398 140467944662912 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 32500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.256575 140467944662912 run_classifier.py:774] Writing example 0 of 32500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.264681 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.267151 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i remember back when i was little when i was away at camp and we would campo ##ut under the stars . there was always someone there that would have a good story to tell that involved the woods that surrounded us and they would always creep me out . well , when i found wen ##di ##go at the library , i checked it out hoping to be one of those films that had a supernatural being haunting people in the woods much like the stories that were told at camp . well , much to my dismay , i was so far from the truth . wen ##di ##go is really bad . the story starts of when a family of three is driving to their winter cabin , which looks like your normal suburban home and nothing like a cabin in the woods , and they run into a deer . well , it seems the local red ##neck ##s were actually hunting this particular deer and are pretty upset at our city folk . the movie spends far too much time following the families everyday activities instead of getting to the point of the film . it wasn ' t until about the last 15 minutes that we actually have some action involving the \" wen ##di ##go . \" my suggestion is that you stay very far away this film . it will leave you wanting your hour and a half back . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.270555 140467944662912 run_classifier.py:464] tokens: [CLS] i remember back when i was little when i was away at camp and we would campo ##ut under the stars . there was always someone there that would have a good story to tell that involved the woods that surrounded us and they would always creep me out . well , when i found wen ##di ##go at the library , i checked it out hoping to be one of those films that had a supernatural being haunting people in the woods much like the stories that were told at camp . well , much to my dismay , i was so far from the truth . wen ##di ##go is really bad . the story starts of when a family of three is driving to their winter cabin , which looks like your normal suburban home and nothing like a cabin in the woods , and they run into a deer . well , it seems the local red ##neck ##s were actually hunting this particular deer and are pretty upset at our city folk . the movie spends far too much time following the families everyday activities instead of getting to the point of the film . it wasn ' t until about the last 15 minutes that we actually have some action involving the \" wen ##di ##go . \" my suggestion is that you stay very far away this film . it will leave you wanting your hour and a half back . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 3342 2067 2043 1045 2001 2210 2043 1045 2001 2185 2012 3409 1998 2057 2052 22339 4904 2104 1996 3340 1012 2045 2001 2467 2619 2045 2008 2052 2031 1037 2204 2466 2000 2425 2008 2920 1996 5249 2008 5129 2149 1998 2027 2052 2467 19815 2033 2041 1012 2092 1010 2043 1045 2179 19181 4305 3995 2012 1996 3075 1010 1045 7039 2009 2041 5327 2000 2022 2028 1997 2216 3152 2008 2018 1037 11189 2108 20161 2111 1999 1996 5249 2172 2066 1996 3441 2008 2020 2409 2012 3409 1012 2092 1010 2172 2000 2026 20006 1010 1045 2001 2061 2521 2013 1996 3606 1012 19181 4305 3995 2003 2428 2919 1012 1996 2466 4627 1997 2043 1037 2155 1997 2093 2003 4439 2000 2037 3467 6644 1010 2029 3504 2066 2115 3671 9282 2188 1998 2498 2066 1037 6644 1999 1996 5249 1010 1998 2027 2448 2046 1037 8448 1012 2092 1010 2009 3849 1996 2334 2417 18278 2015 2020 2941 5933 2023 3327 8448 1998 2024 3492 6314 2012 2256 2103 5154 1012 1996 3185 15970 2521 2205 2172 2051 2206 1996 2945 10126 3450 2612 1997 2893 2000 1996 2391 1997 1996 2143 1012 2009 2347 1005 1056 2127 2055 1996 2197 2321 2781 2008 2057 2941 2031 2070 2895 5994 1996 1000 19181 4305 3995 1012 1000 2026 10293 2003 2008 2017 2994 2200 2521 2185 2023 2143 1012 2009 2097 2681 2017 5782 2115 3178 1998 1037 2431 2067 1012 102 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.273000 140467944662912 run_classifier.py:465] input_ids: 101 1045 3342 2067 2043 1045 2001 2210 2043 1045 2001 2185 2012 3409 1998 2057 2052 22339 4904 2104 1996 3340 1012 2045 2001 2467 2619 2045 2008 2052 2031 1037 2204 2466 2000 2425 2008 2920 1996 5249 2008 5129 2149 1998 2027 2052 2467 19815 2033 2041 1012 2092 1010 2043 1045 2179 19181 4305 3995 2012 1996 3075 1010 1045 7039 2009 2041 5327 2000 2022 2028 1997 2216 3152 2008 2018 1037 11189 2108 20161 2111 1999 1996 5249 2172 2066 1996 3441 2008 2020 2409 2012 3409 1012 2092 1010 2172 2000 2026 20006 1010 1045 2001 2061 2521 2013 1996 3606 1012 19181 4305 3995 2003 2428 2919 1012 1996 2466 4627 1997 2043 1037 2155 1997 2093 2003 4439 2000 2037 3467 6644 1010 2029 3504 2066 2115 3671 9282 2188 1998 2498 2066 1037 6644 1999 1996 5249 1010 1998 2027 2448 2046 1037 8448 1012 2092 1010 2009 3849 1996 2334 2417 18278 2015 2020 2941 5933 2023 3327 8448 1998 2024 3492 6314 2012 2256 2103 5154 1012 1996 3185 15970 2521 2205 2172 2051 2206 1996 2945 10126 3450 2612 1997 2893 2000 1996 2391 1997 1996 2143 1012 2009 2347 1005 1056 2127 2055 1996 2197 2321 2781 2008 2057 2941 2031 2070 2895 5994 1996 1000 19181 4305 3995 1012 1000 2026 10293 2003 2008 2017 2994 2200 2521 2185 2023 2143 1012 2009 2097 2681 2017 5782 2115 3178 1998 1037 2431 2067 1012 102 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.276044 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.279091 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.282056 140467944662912 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.300014 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.302469 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i ' ve waited a long time to see dr tar ##r ' s torture dungeon and after i watched it , i was really disappointed by it . it ' s not the baroque film i expected it to be . the trailer ( which i saw on a something weird dvd ) is much better than the entire film , which is remarkably forget ##table . there are almost no stand out scenes in it and the look and feel is interesting but it doesn ' t even come close to other baroque styled movies out there , from fell ##ini or jo ##dor ##ows ##ky . the characters are dull and there ' s almost nothing dramatic going on , even though we see rape , cr ##uc ##if ##ix ##ion , insanity , etc . < br / > < br / > the main problem with dr tar ##r ' s torture dungeon was the fact that it was a talk - a - tho ##n more than anything else . it was almost like watching a book . i just wanted the film to have moments of silence or mood or something , instead we see / listen to the main characters chi ##t - chat endless ##ly about dull stuff . < br / > < br / > a missed opportunity . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.304534 140467944662912 run_classifier.py:464] tokens: [CLS] i ' ve waited a long time to see dr tar ##r ' s torture dungeon and after i watched it , i was really disappointed by it . it ' s not the baroque film i expected it to be . the trailer ( which i saw on a something weird dvd ) is much better than the entire film , which is remarkably forget ##table . there are almost no stand out scenes in it and the look and feel is interesting but it doesn ' t even come close to other baroque styled movies out there , from fell ##ini or jo ##dor ##ows ##ky . the characters are dull and there ' s almost nothing dramatic going on , even though we see rape , cr ##uc ##if ##ix ##ion , insanity , etc . < br / > < br / > the main problem with dr tar ##r ' s torture dungeon was the fact that it was a talk - a - tho ##n more than anything else . it was almost like watching a book . i just wanted the film to have moments of silence or mood or something , instead we see / listen to the main characters chi ##t - chat endless ##ly about dull stuff . < br / > < br / > a missed opportunity . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 1005 2310 4741 1037 2146 2051 2000 2156 2852 16985 2099 1005 1055 8639 16633 1998 2044 1045 3427 2009 1010 1045 2001 2428 9364 2011 2009 1012 2009 1005 1055 2025 1996 10456 2143 1045 3517 2009 2000 2022 1012 1996 9117 1006 2029 1045 2387 2006 1037 2242 6881 4966 1007 2003 2172 2488 2084 1996 2972 2143 1010 2029 2003 17431 5293 10880 1012 2045 2024 2471 2053 3233 2041 5019 1999 2009 1998 1996 2298 1998 2514 2003 5875 2021 2009 2987 1005 1056 2130 2272 2485 2000 2060 10456 13650 5691 2041 2045 1010 2013 3062 5498 2030 8183 7983 15568 4801 1012 1996 3494 2024 10634 1998 2045 1005 1055 2471 2498 6918 2183 2006 1010 2130 2295 2057 2156 9040 1010 13675 14194 10128 7646 3258 1010 19272 1010 4385 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2364 3291 2007 2852 16985 2099 1005 1055 8639 16633 2001 1996 2755 2008 2009 2001 1037 2831 1011 1037 1011 27793 2078 2062 2084 2505 2842 1012 2009 2001 2471 2066 3666 1037 2338 1012 1045 2074 2359 1996 2143 2000 2031 5312 1997 4223 2030 6888 2030 2242 1010 2612 2057 2156 1013 4952 2000 1996 2364 3494 9610 2102 1011 11834 10866 2135 2055 10634 4933 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 4771 4495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.306436 140467944662912 run_classifier.py:465] input_ids: 101 1045 1005 2310 4741 1037 2146 2051 2000 2156 2852 16985 2099 1005 1055 8639 16633 1998 2044 1045 3427 2009 1010 1045 2001 2428 9364 2011 2009 1012 2009 1005 1055 2025 1996 10456 2143 1045 3517 2009 2000 2022 1012 1996 9117 1006 2029 1045 2387 2006 1037 2242 6881 4966 1007 2003 2172 2488 2084 1996 2972 2143 1010 2029 2003 17431 5293 10880 1012 2045 2024 2471 2053 3233 2041 5019 1999 2009 1998 1996 2298 1998 2514 2003 5875 2021 2009 2987 1005 1056 2130 2272 2485 2000 2060 10456 13650 5691 2041 2045 1010 2013 3062 5498 2030 8183 7983 15568 4801 1012 1996 3494 2024 10634 1998 2045 1005 1055 2471 2498 6918 2183 2006 1010 2130 2295 2057 2156 9040 1010 13675 14194 10128 7646 3258 1010 19272 1010 4385 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2364 3291 2007 2852 16985 2099 1005 1055 8639 16633 2001 1996 2755 2008 2009 2001 1037 2831 1011 1037 1011 27793 2078 2062 2084 2505 2842 1012 2009 2001 2471 2066 3666 1037 2338 1012 1045 2074 2359 1996 2143 2000 2031 5312 1997 4223 2030 6888 2030 2242 1010 2612 2057 2156 1013 4952 2000 1996 2364 3494 9610 2102 1011 11834 10866 2135 2055 10634 4933 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 4771 4495 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.308275 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.312040 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.314832 140467944662912 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.321009 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.322920 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i enjoyed the beautiful scenery in this movie the first time i saw it when i was 9 . dun ##der ##k ##lum ##pen is kind of cute for kidd ##ies in a corn ##y way . it reminded me of hr ##pu ##ffin ##st ##uf ##f on sat mornings , its swedish backdrop ##s make it easy on the eyes . don ' t expect older kids to be interested as the live action / animation is way behind the times and most older kids will get bored . this is definitely an under 10 age set movie and a nice bit of memories for those of us who were little kids in 1974 . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.325356 140467944662912 run_classifier.py:464] tokens: [CLS] i enjoyed the beautiful scenery in this movie the first time i saw it when i was 9 . dun ##der ##k ##lum ##pen is kind of cute for kidd ##ies in a corn ##y way . it reminded me of hr ##pu ##ffin ##st ##uf ##f on sat mornings , its swedish backdrop ##s make it easy on the eyes . don ' t expect older kids to be interested as the live action / animation is way behind the times and most older kids will get bored . this is definitely an under 10 age set movie and a nice bit of memories for those of us who were little kids in 1974 . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 5632 1996 3376 17363 1999 2023 3185 1996 2034 2051 1045 2387 2009 2043 1045 2001 1023 1012 24654 4063 2243 12942 11837 2003 2785 1997 10140 2005 25358 3111 1999 1037 9781 2100 2126 1012 2009 6966 2033 1997 17850 14289 15379 3367 16093 2546 2006 2938 16956 1010 2049 4467 18876 2015 2191 2009 3733 2006 1996 2159 1012 2123 1005 1056 5987 3080 4268 2000 2022 4699 2004 1996 2444 2895 1013 7284 2003 2126 2369 1996 2335 1998 2087 3080 4268 2097 2131 11471 1012 2023 2003 5791 2019 2104 2184 2287 2275 3185 1998 1037 3835 2978 1997 5758 2005 2216 1997 2149 2040 2020 2210 4268 1999 3326 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.327850 140467944662912 run_classifier.py:465] input_ids: 101 1045 5632 1996 3376 17363 1999 2023 3185 1996 2034 2051 1045 2387 2009 2043 1045 2001 1023 1012 24654 4063 2243 12942 11837 2003 2785 1997 10140 2005 25358 3111 1999 1037 9781 2100 2126 1012 2009 6966 2033 1997 17850 14289 15379 3367 16093 2546 2006 2938 16956 1010 2049 4467 18876 2015 2191 2009 3733 2006 1996 2159 1012 2123 1005 1056 5987 3080 4268 2000 2022 4699 2004 1996 2444 2895 1013 7284 2003 2126 2369 1996 2335 1998 2087 3080 4268 2097 2131 11471 1012 2023 2003 5791 2019 2104 2184 2287 2275 3185 1998 1037 3835 2978 1997 5758 2005 2216 1997 2149 2040 2020 2210 4268 1999 3326 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.329885 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.331880 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.334220 140467944662912 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.340612 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.342909 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] it ' s a good show , and i find it funny . finally the bad latin stereo types are over ! aa ¡ gr ##ac ##ias , sea ##a ##± ##or lopez ! i love this show , and i just started watching it about three months ago . the whole concept about a latin family tv show really amazed me . i am surprised that finally latino ##s have a good shot to be on tv . this show is probably one the best i ' ve seen , it ' s funny , heart ##war ##ming , touch ##y , and nice . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.344835 140467944662912 run_classifier.py:464] tokens: [CLS] it ' s a good show , and i find it funny . finally the bad latin stereo types are over ! aa ¡ gr ##ac ##ias , sea ##a ##± ##or lopez ! i love this show , and i just started watching it about three months ago . the whole concept about a latin family tv show really amazed me . i am surprised that finally latino ##s have a good shot to be on tv . this show is probably one the best i ' ve seen , it ' s funny , heart ##war ##ming , touch ##y , and nice . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2009 1005 1055 1037 2204 2265 1010 1998 1045 2424 2009 6057 1012 2633 1996 2919 3763 12991 4127 2024 2058 999 9779 1067 24665 6305 7951 1010 2712 2050 29657 2953 8685 999 1045 2293 2023 2265 1010 1998 1045 2074 2318 3666 2009 2055 2093 2706 3283 1012 1996 2878 4145 2055 1037 3763 2155 2694 2265 2428 15261 2033 1012 1045 2572 4527 2008 2633 7402 2015 2031 1037 2204 2915 2000 2022 2006 2694 1012 2023 2265 2003 2763 2028 1996 2190 1045 1005 2310 2464 1010 2009 1005 1055 6057 1010 2540 9028 6562 1010 3543 2100 1010 1998 3835 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.347253 140467944662912 run_classifier.py:465] input_ids: 101 2009 1005 1055 1037 2204 2265 1010 1998 1045 2424 2009 6057 1012 2633 1996 2919 3763 12991 4127 2024 2058 999 9779 1067 24665 6305 7951 1010 2712 2050 29657 2953 8685 999 1045 2293 2023 2265 1010 1998 1045 2074 2318 3666 2009 2055 2093 2706 3283 1012 1996 2878 4145 2055 1037 3763 2155 2694 2265 2428 15261 2033 1012 1045 2572 4527 2008 2633 7402 2015 2031 1037 2204 2915 2000 2022 2006 2694 1012 2023 2265 2003 2763 2028 1996 2190 1045 1005 2310 2464 1010 2009 1005 1055 6057 1010 2540 9028 6562 1010 3543 2100 1010 1998 3835 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.349702 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.351773 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.354087 140467944662912 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.375899 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.378335 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the penultimate episode of star trek ' s third season is excellent and a highlight of the much mali ##gned final season . essentially , sp ##ock , mccoy and kirk beam down to sar ##pe ##idon to find the planet ' s population completely missing except for the presence of a giant library and mr . at ##oz , the librarian . all 3 trek characters soon accidentally walk into a time travel machine into different periods of sar ##pe ##idon ' s past . sp ##ock gives a convincing performance as an ice age vulcan who falls in love for za ##ra ##bet ##h while kirk reprise ##s his unhappy experience with time travel - - see the ' city on the edge of forever ' - - when he is accused of witchcraft and jailed before escaping and finding the doorway back in time to sar ##pe ##idon ' s present . in the end , all 3 trek characters are saved mere minutes before the beta ni ##obe star around sar ##pe ##idon goes super ##nova . the enterprise warp ##s away just as the star explodes . < br / > < br / > ironically , as william sha ##tner notes in his book \" star trek memories , \" this show was the source of some dispute since leonard ni ##mo ##y noticed that no reason was given in li ##sett ##e ' s script for the reason why sp ##ock was be ##ha ##ving in such [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.380841 140467944662912 run_classifier.py:464] tokens: [CLS] the penultimate episode of star trek ' s third season is excellent and a highlight of the much mali ##gned final season . essentially , sp ##ock , mccoy and kirk beam down to sar ##pe ##idon to find the planet ' s population completely missing except for the presence of a giant library and mr . at ##oz , the librarian . all 3 trek characters soon accidentally walk into a time travel machine into different periods of sar ##pe ##idon ' s past . sp ##ock gives a convincing performance as an ice age vulcan who falls in love for za ##ra ##bet ##h while kirk reprise ##s his unhappy experience with time travel - - see the ' city on the edge of forever ' - - when he is accused of witchcraft and jailed before escaping and finding the doorway back in time to sar ##pe ##idon ' s present . in the end , all 3 trek characters are saved mere minutes before the beta ni ##obe star around sar ##pe ##idon goes super ##nova . the enterprise warp ##s away just as the star explodes . < br / > < br / > ironically , as william sha ##tner notes in his book \" star trek memories , \" this show was the source of some dispute since leonard ni ##mo ##y noticed that no reason was given in li ##sett ##e ' s script for the reason why sp ##ock was be ##ha ##ving in such [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 25512 2792 1997 2732 10313 1005 1055 2353 2161 2003 6581 1998 1037 12944 1997 1996 2172 16007 19225 2345 2161 1012 7687 1010 11867 7432 1010 16075 1998 11332 7504 2091 2000 18906 5051 28201 2000 2424 1996 4774 1005 1055 2313 3294 4394 3272 2005 1996 3739 1997 1037 5016 3075 1998 2720 1012 2012 18153 1010 1996 13850 1012 2035 1017 10313 3494 2574 9554 3328 2046 1037 2051 3604 3698 2046 2367 6993 1997 18906 5051 28201 1005 1055 2627 1012 11867 7432 3957 1037 13359 2836 2004 2019 3256 2287 25993 2040 4212 1999 2293 2005 23564 2527 20915 2232 2096 11332 16851 2015 2010 12511 3325 2007 2051 3604 1011 1011 2156 1996 1005 2103 2006 1996 3341 1997 5091 1005 1011 1011 2043 2002 2003 5496 1997 21599 1998 21278 2077 13002 1998 4531 1996 7086 2067 1999 2051 2000 18906 5051 28201 1005 1055 2556 1012 1999 1996 2203 1010 2035 1017 10313 3494 2024 5552 8210 2781 2077 1996 8247 9152 20891 2732 2105 18906 5051 28201 3632 3565 13455 1012 1996 6960 24136 2015 2185 2074 2004 1996 2732 27583 1012 1026 7987 1013 1028 1026 7987 1013 1028 18527 1010 2004 2520 21146 18885 3964 1999 2010 2338 1000 2732 10313 5758 1010 1000 2023 2265 2001 1996 3120 1997 2070 7593 2144 7723 9152 5302 2100 4384 2008 2053 3114 2001 2445 1999 5622 21678 2063 1005 1055 5896 2005 1996 3114 2339 11867 7432 2001 2022 3270 6455 1999 2107 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.383301 140467944662912 run_classifier.py:465] input_ids: 101 1996 25512 2792 1997 2732 10313 1005 1055 2353 2161 2003 6581 1998 1037 12944 1997 1996 2172 16007 19225 2345 2161 1012 7687 1010 11867 7432 1010 16075 1998 11332 7504 2091 2000 18906 5051 28201 2000 2424 1996 4774 1005 1055 2313 3294 4394 3272 2005 1996 3739 1997 1037 5016 3075 1998 2720 1012 2012 18153 1010 1996 13850 1012 2035 1017 10313 3494 2574 9554 3328 2046 1037 2051 3604 3698 2046 2367 6993 1997 18906 5051 28201 1005 1055 2627 1012 11867 7432 3957 1037 13359 2836 2004 2019 3256 2287 25993 2040 4212 1999 2293 2005 23564 2527 20915 2232 2096 11332 16851 2015 2010 12511 3325 2007 2051 3604 1011 1011 2156 1996 1005 2103 2006 1996 3341 1997 5091 1005 1011 1011 2043 2002 2003 5496 1997 21599 1998 21278 2077 13002 1998 4531 1996 7086 2067 1999 2051 2000 18906 5051 28201 1005 1055 2556 1012 1999 1996 2203 1010 2035 1017 10313 3494 2024 5552 8210 2781 2077 1996 8247 9152 20891 2732 2105 18906 5051 28201 3632 3565 13455 1012 1996 6960 24136 2015 2185 2074 2004 1996 2732 27583 1012 1026 7987 1013 1028 1026 7987 1013 1028 18527 1010 2004 2520 21146 18885 3964 1999 2010 2338 1000 2732 10313 5758 1010 1000 2023 2265 2001 1996 3120 1997 2070 7593 2144 7723 9152 5302 2100 4384 2008 2053 3114 2001 2445 1999 5622 21678 2063 1005 1055 5896 2005 1996 3114 2339 11867 7432 2001 2022 3270 6455 1999 2107 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.385535 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.388028 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:23:22.389891 140467944662912 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 10000 of 32500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:24:10.804726 140467944662912 run_classifier.py:774] Writing example 10000 of 32500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 20000 of 32500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:24:57.775064 140467944662912 run_classifier.py:774] Writing example 20000 of 32500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 30000 of 32500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:46.355795 140467944662912 run_classifier.py:774] Writing example 30000 of 32500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.229934 140467944662912 run_classifier.py:774] Writing example 0 of 10000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.236339 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.238430 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i went and saw this movie last night after being coa ##xed to by a few friends of mine . i ' ll admit that i was reluctant to see it because from what i knew of ashton ku ##tch ##er he was only able to do comedy . i was wrong . ku ##tch ##er played the character of jake fischer very well , and kevin cost ##ner played ben randall with such professional ##ism . the sign of a good movie is that it can toy with our emotions . this one did exactly that . the entire theater ( which was sold out ) was overcome by laughter during the first half of the movie , and were moved to tears during the second half . while exiting the theater i not only saw many women in tears , but many full grown men as well , trying desperately not to let anyone see them crying . this movie was great , and i suggest that you go see it before you judge . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.240115 140467944662912 run_classifier.py:464] tokens: [CLS] i went and saw this movie last night after being coa ##xed to by a few friends of mine . i ' ll admit that i was reluctant to see it because from what i knew of ashton ku ##tch ##er he was only able to do comedy . i was wrong . ku ##tch ##er played the character of jake fischer very well , and kevin cost ##ner played ben randall with such professional ##ism . the sign of a good movie is that it can toy with our emotions . this one did exactly that . the entire theater ( which was sold out ) was overcome by laughter during the first half of the movie , and were moved to tears during the second half . while exiting the theater i not only saw many women in tears , but many full grown men as well , trying desperately not to let anyone see them crying . this movie was great , and i suggest that you go see it before you judge . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 2253 1998 2387 2023 3185 2197 2305 2044 2108 28155 19068 2000 2011 1037 2261 2814 1997 3067 1012 1045 1005 2222 6449 2008 1045 2001 11542 2000 2156 2009 2138 2013 2054 1045 2354 1997 13772 13970 10649 2121 2002 2001 2069 2583 2000 2079 4038 1012 1045 2001 3308 1012 13970 10649 2121 2209 1996 2839 1997 5180 13042 2200 2092 1010 1998 4901 3465 3678 2209 3841 12813 2007 2107 2658 2964 1012 1996 3696 1997 1037 2204 3185 2003 2008 2009 2064 9121 2007 2256 6699 1012 2023 2028 2106 3599 2008 1012 1996 2972 4258 1006 2029 2001 2853 2041 1007 2001 9462 2011 7239 2076 1996 2034 2431 1997 1996 3185 1010 1998 2020 2333 2000 4000 2076 1996 2117 2431 1012 2096 22371 1996 4258 1045 2025 2069 2387 2116 2308 1999 4000 1010 2021 2116 2440 4961 2273 2004 2092 1010 2667 9652 2025 2000 2292 3087 2156 2068 6933 1012 2023 3185 2001 2307 1010 1998 1045 6592 2008 2017 2175 2156 2009 2077 2017 3648 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.241517 140467944662912 run_classifier.py:465] input_ids: 101 1045 2253 1998 2387 2023 3185 2197 2305 2044 2108 28155 19068 2000 2011 1037 2261 2814 1997 3067 1012 1045 1005 2222 6449 2008 1045 2001 11542 2000 2156 2009 2138 2013 2054 1045 2354 1997 13772 13970 10649 2121 2002 2001 2069 2583 2000 2079 4038 1012 1045 2001 3308 1012 13970 10649 2121 2209 1996 2839 1997 5180 13042 2200 2092 1010 1998 4901 3465 3678 2209 3841 12813 2007 2107 2658 2964 1012 1996 3696 1997 1037 2204 3185 2003 2008 2009 2064 9121 2007 2256 6699 1012 2023 2028 2106 3599 2008 1012 1996 2972 4258 1006 2029 2001 2853 2041 1007 2001 9462 2011 7239 2076 1996 2034 2431 1997 1996 3185 1010 1998 2020 2333 2000 4000 2076 1996 2117 2431 1012 2096 22371 1996 4258 1045 2025 2069 2387 2116 2308 1999 4000 1010 2021 2116 2440 4961 2273 2004 2092 1010 2667 9652 2025 2000 2292 3087 2156 2068 6933 1012 2023 3185 2001 2307 1010 1998 1045 6592 2008 2017 2175 2156 2009 2077 2017 3648 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.243418 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.246340 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.250366 140467944662912 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.266266 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.268258 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] actor turned director bill paxton follows up his promising debut , the gothic - horror \" frail ##ty \" , with this family friendly sports drama about the 1913 u . s . open where a young american cad ##dy rises from his humble background to play against his br ##ist ##ish idol in what was dubbed as \" the greatest game ever played . \" i ' m no fan of golf , and these scrap ##py under ##dog sports flick ##s are a dime a dozen ( most recently done to grand effect with \" miracle \" and \" cinderella man \" ) , but some how this film was en ##th ##ral ##ling all the same . < br / > < br / > the film starts with some creative opening credits ( imagine a disney ##fied version of the animated opening credits of hbo ' s \" carnival ##e \" and \" rome \" ) , but lumber ##s along slowly for its first by - the - numbers hour . once the action moves to the u . s . open things pick up very well . paxton does a nice job and shows a kn ##ack for effective directorial flourish ##es ( i loved the rain - soaked mont ##age of the action on day two of the open ) that prop ##el the plot further or add some unexpected psychological depth to the proceedings . there ' s some compelling character development when the british harry [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.270486 140467944662912 run_classifier.py:464] tokens: [CLS] actor turned director bill paxton follows up his promising debut , the gothic - horror \" frail ##ty \" , with this family friendly sports drama about the 1913 u . s . open where a young american cad ##dy rises from his humble background to play against his br ##ist ##ish idol in what was dubbed as \" the greatest game ever played . \" i ' m no fan of golf , and these scrap ##py under ##dog sports flick ##s are a dime a dozen ( most recently done to grand effect with \" miracle \" and \" cinderella man \" ) , but some how this film was en ##th ##ral ##ling all the same . < br / > < br / > the film starts with some creative opening credits ( imagine a disney ##fied version of the animated opening credits of hbo ' s \" carnival ##e \" and \" rome \" ) , but lumber ##s along slowly for its first by - the - numbers hour . once the action moves to the u . s . open things pick up very well . paxton does a nice job and shows a kn ##ack for effective directorial flourish ##es ( i loved the rain - soaked mont ##age of the action on day two of the open ) that prop ##el the plot further or add some unexpected psychological depth to the proceedings . there ' s some compelling character development when the british harry [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3364 2357 2472 3021 27765 4076 2039 2010 10015 2834 1010 1996 7788 1011 5469 1000 25737 3723 1000 1010 2007 2023 2155 5379 2998 3689 2055 1996 5124 1057 1012 1055 1012 2330 2073 1037 2402 2137 28353 5149 9466 2013 2010 15716 4281 2000 2377 2114 2010 7987 2923 4509 10282 1999 2054 2001 9188 2004 1000 1996 4602 2208 2412 2209 1012 1000 1045 1005 1049 2053 5470 1997 5439 1010 1998 2122 15121 7685 2104 16168 2998 17312 2015 2024 1037 27211 1037 6474 1006 2087 3728 2589 2000 2882 3466 2007 1000 9727 1000 1998 1000 21686 2158 1000 1007 1010 2021 2070 2129 2023 2143 2001 4372 2705 7941 2989 2035 1996 2168 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2143 4627 2007 2070 5541 3098 6495 1006 5674 1037 6373 10451 2544 1997 1996 6579 3098 6495 1997 14633 1005 1055 1000 11485 2063 1000 1998 1000 4199 1000 1007 1010 2021 13891 2015 2247 3254 2005 2049 2034 2011 1011 1996 1011 3616 3178 1012 2320 1996 2895 5829 2000 1996 1057 1012 1055 1012 2330 2477 4060 2039 2200 2092 1012 27765 2515 1037 3835 3105 1998 3065 1037 14161 8684 2005 4621 21635 24299 2229 1006 1045 3866 1996 4542 1011 13077 18318 4270 1997 1996 2895 2006 2154 2048 1997 1996 2330 1007 2008 17678 2884 1996 5436 2582 2030 5587 2070 9223 8317 5995 2000 1996 8931 1012 2045 1005 1055 2070 17075 2839 2458 2043 1996 2329 4302 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.272535 140467944662912 run_classifier.py:465] input_ids: 101 3364 2357 2472 3021 27765 4076 2039 2010 10015 2834 1010 1996 7788 1011 5469 1000 25737 3723 1000 1010 2007 2023 2155 5379 2998 3689 2055 1996 5124 1057 1012 1055 1012 2330 2073 1037 2402 2137 28353 5149 9466 2013 2010 15716 4281 2000 2377 2114 2010 7987 2923 4509 10282 1999 2054 2001 9188 2004 1000 1996 4602 2208 2412 2209 1012 1000 1045 1005 1049 2053 5470 1997 5439 1010 1998 2122 15121 7685 2104 16168 2998 17312 2015 2024 1037 27211 1037 6474 1006 2087 3728 2589 2000 2882 3466 2007 1000 9727 1000 1998 1000 21686 2158 1000 1007 1010 2021 2070 2129 2023 2143 2001 4372 2705 7941 2989 2035 1996 2168 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2143 4627 2007 2070 5541 3098 6495 1006 5674 1037 6373 10451 2544 1997 1996 6579 3098 6495 1997 14633 1005 1055 1000 11485 2063 1000 1998 1000 4199 1000 1007 1010 2021 13891 2015 2247 3254 2005 2049 2034 2011 1011 1996 1011 3616 3178 1012 2320 1996 2895 5829 2000 1996 1057 1012 1055 1012 2330 2477 4060 2039 2200 2092 1012 27765 2515 1037 3835 3105 1998 3065 1037 14161 8684 2005 4621 21635 24299 2229 1006 1045 3866 1996 4542 1011 13077 18318 4270 1997 1996 2895 2006 2154 2048 1997 1996 2330 1007 2008 17678 2884 1996 5436 2582 2030 5587 2070 9223 8317 5995 2000 1996 8931 1012 2045 1005 1055 2070 17075 2839 2458 2043 1996 2329 4302 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.274939 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.277164 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.279012 140467944662912 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.289665 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.291925 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] as a recreational golfer with some knowledge of the sport ' s history , i was pleased with disney ' s sensitivity to the issues of class in golf in the early twentieth century . the movie depicted well the psychological battles that harry var ##don fought within himself , from his childhood trauma of being evicted to his own inability to break that glass ceiling that prevents him from being accepted as an equal in english golf society . likewise , the young ou ##ime ##t goes through his own class struggles , being a mere cad ##die in the eyes of the upper crust americans who sc ##off at his attempts to rise above his standing . < br / > < br / > what i loved best , however , is how this theme of class is manifested in the characters of ou ##ime ##t ' s parents . his father is a working - class drone who sees the value of hard work but is intimidated by the upper class ; his mother , however , recognizes her son ' s talent and desire and encourages him to pursue his dream of competing against those who think he is inferior . < br / > < br / > finally , the golf scenes are well photographed . although the course used in the movie was not the actual site of the historical tournament , the little liberties taken by disney do not det ##rac ##t from the beauty [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.294000 140467944662912 run_classifier.py:464] tokens: [CLS] as a recreational golfer with some knowledge of the sport ' s history , i was pleased with disney ' s sensitivity to the issues of class in golf in the early twentieth century . the movie depicted well the psychological battles that harry var ##don fought within himself , from his childhood trauma of being evicted to his own inability to break that glass ceiling that prevents him from being accepted as an equal in english golf society . likewise , the young ou ##ime ##t goes through his own class struggles , being a mere cad ##die in the eyes of the upper crust americans who sc ##off at his attempts to rise above his standing . < br / > < br / > what i loved best , however , is how this theme of class is manifested in the characters of ou ##ime ##t ' s parents . his father is a working - class drone who sees the value of hard work but is intimidated by the upper class ; his mother , however , recognizes her son ' s talent and desire and encourages him to pursue his dream of competing against those who think he is inferior . < br / > < br / > finally , the golf scenes are well photographed . although the course used in the movie was not the actual site of the historical tournament , the little liberties taken by disney do not det ##rac ##t from the beauty [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2004 1037 10517 20601 2007 2070 3716 1997 1996 4368 1005 1055 2381 1010 1045 2001 7537 2007 6373 1005 1055 14639 2000 1996 3314 1997 2465 1999 5439 1999 1996 2220 9086 2301 1012 1996 3185 8212 2092 1996 8317 7465 2008 4302 13075 5280 4061 2306 2370 1010 2013 2010 5593 12603 1997 2108 25777 2000 2010 2219 13720 2000 3338 2008 3221 5894 2008 16263 2032 2013 2108 3970 2004 2019 5020 1999 2394 5439 2554 1012 10655 1010 1996 2402 15068 14428 2102 3632 2083 2010 2219 2465 11785 1010 2108 1037 8210 28353 10265 1999 1996 2159 1997 1996 3356 19116 4841 2040 8040 7245 2012 2010 4740 2000 4125 2682 2010 3061 1012 1026 7987 1013 1028 1026 7987 1013 1028 2054 1045 3866 2190 1010 2174 1010 2003 2129 2023 4323 1997 2465 2003 24906 1999 1996 3494 1997 15068 14428 2102 1005 1055 3008 1012 2010 2269 2003 1037 2551 1011 2465 18465 2040 5927 1996 3643 1997 2524 2147 2021 2003 28028 2011 1996 3356 2465 1025 2010 2388 1010 2174 1010 14600 2014 2365 1005 1055 5848 1998 4792 1998 16171 2032 2000 7323 2010 3959 1997 6637 2114 2216 2040 2228 2002 2003 14092 1012 1026 7987 1013 1028 1026 7987 1013 1028 2633 1010 1996 5439 5019 2024 2092 16164 1012 2348 1996 2607 2109 1999 1996 3185 2001 2025 1996 5025 2609 1997 1996 3439 2977 1010 1996 2210 18271 2579 2011 6373 2079 2025 20010 22648 2102 2013 1996 5053 102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.296442 140467944662912 run_classifier.py:465] input_ids: 101 2004 1037 10517 20601 2007 2070 3716 1997 1996 4368 1005 1055 2381 1010 1045 2001 7537 2007 6373 1005 1055 14639 2000 1996 3314 1997 2465 1999 5439 1999 1996 2220 9086 2301 1012 1996 3185 8212 2092 1996 8317 7465 2008 4302 13075 5280 4061 2306 2370 1010 2013 2010 5593 12603 1997 2108 25777 2000 2010 2219 13720 2000 3338 2008 3221 5894 2008 16263 2032 2013 2108 3970 2004 2019 5020 1999 2394 5439 2554 1012 10655 1010 1996 2402 15068 14428 2102 3632 2083 2010 2219 2465 11785 1010 2108 1037 8210 28353 10265 1999 1996 2159 1997 1996 3356 19116 4841 2040 8040 7245 2012 2010 4740 2000 4125 2682 2010 3061 1012 1026 7987 1013 1028 1026 7987 1013 1028 2054 1045 3866 2190 1010 2174 1010 2003 2129 2023 4323 1997 2465 2003 24906 1999 1996 3494 1997 15068 14428 2102 1005 1055 3008 1012 2010 2269 2003 1037 2551 1011 2465 18465 2040 5927 1996 3643 1997 2524 2147 2021 2003 28028 2011 1996 3356 2465 1025 2010 2388 1010 2174 1010 14600 2014 2365 1005 1055 5848 1998 4792 1998 16171 2032 2000 7323 2010 3959 1997 6637 2114 2216 2040 2228 2002 2003 14092 1012 1026 7987 1013 1028 1026 7987 1013 1028 2633 1010 1996 5439 5019 2024 2092 16164 1012 2348 1996 2607 2109 1999 1996 3185 2001 2025 1996 5025 2609 1997 1996 3439 2977 1010 1996 2210 18271 2579 2011 6373 2079 2025 20010 22648 2102 2013 1996 5053 102\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.298776 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.301021 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.302854 140467944662912 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.310074 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.312117 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i saw this film in a sneak preview , and it is delightful . the cinematography is unusually creative , the acting is good , and the story is fabulous . if this movie does not do well , it won ' t be because it doesn ' t deserve to . before this film , i didn ' t realize how charming shia le ##bo ##uf could be . he does a marvelous , self - contained , job as the lead . there ' s something incredibly sweet about him , and it makes the movie even better . the other actors do a good job as well , and the film contains moments of really high suspense , more than one might expect from a movie about golf . sports movies are a dime a dozen , but this one stands out . < br / > < br / > this is one i ' d recommend to anyone . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.314076 140467944662912 run_classifier.py:464] tokens: [CLS] i saw this film in a sneak preview , and it is delightful . the cinematography is unusually creative , the acting is good , and the story is fabulous . if this movie does not do well , it won ' t be because it doesn ' t deserve to . before this film , i didn ' t realize how charming shia le ##bo ##uf could be . he does a marvelous , self - contained , job as the lead . there ' s something incredibly sweet about him , and it makes the movie even better . the other actors do a good job as well , and the film contains moments of really high suspense , more than one might expect from a movie about golf . sports movies are a dime a dozen , but this one stands out . < br / > < br / > this is one i ' d recommend to anyone . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 2387 2023 2143 1999 1037 13583 19236 1010 1998 2009 2003 26380 1012 1996 16434 2003 12890 5541 1010 1996 3772 2003 2204 1010 1998 1996 2466 2003 18783 1012 2065 2023 3185 2515 2025 2079 2092 1010 2009 2180 1005 1056 2022 2138 2009 2987 1005 1056 10107 2000 1012 2077 2023 2143 1010 1045 2134 1005 1056 5382 2129 11951 20474 3393 5092 16093 2071 2022 1012 2002 2515 1037 28851 1010 2969 1011 4838 1010 3105 2004 1996 2599 1012 2045 1005 1055 2242 11757 4086 2055 2032 1010 1998 2009 3084 1996 3185 2130 2488 1012 1996 2060 5889 2079 1037 2204 3105 2004 2092 1010 1998 1996 2143 3397 5312 1997 2428 2152 23873 1010 2062 2084 2028 2453 5987 2013 1037 3185 2055 5439 1012 2998 5691 2024 1037 27211 1037 6474 1010 2021 2023 2028 4832 2041 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2003 2028 1045 1005 1040 16755 2000 3087 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.316301 140467944662912 run_classifier.py:465] input_ids: 101 1045 2387 2023 2143 1999 1037 13583 19236 1010 1998 2009 2003 26380 1012 1996 16434 2003 12890 5541 1010 1996 3772 2003 2204 1010 1998 1996 2466 2003 18783 1012 2065 2023 3185 2515 2025 2079 2092 1010 2009 2180 1005 1056 2022 2138 2009 2987 1005 1056 10107 2000 1012 2077 2023 2143 1010 1045 2134 1005 1056 5382 2129 11951 20474 3393 5092 16093 2071 2022 1012 2002 2515 1037 28851 1010 2969 1011 4838 1010 3105 2004 1996 2599 1012 2045 1005 1055 2242 11757 4086 2055 2032 1010 1998 2009 3084 1996 3185 2130 2488 1012 1996 2060 5889 2079 1037 2204 3105 2004 2092 1010 1998 1996 2143 3397 5312 1997 2428 2152 23873 1010 2062 2084 2028 2453 5987 2013 1037 3185 2055 5439 1012 2998 5691 2024 1037 27211 1037 6474 1010 2021 2023 2028 4832 2041 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2003 2028 1045 1005 1040 16755 2000 3087 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.318516 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.320452 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.322284 140467944662912 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.331753 140467944662912 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.333853 140467944662912 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] bill paxton has taken the true story of the 1913 us golf open and made a film that is about much more than an extra - ordinary game of golf . the film also deals directly with the class tensions of the early twentieth century and touches upon the profound anti - catholic prejudice ##s of both the british and american establishments . but at heart the film is about that perennial favourite of triumph against the odds . < br / > < br / > the acting is exemplary throughout . stephen dil ##lane is excellent as usual , but the revelation of the movie is shia lab ##oe ##uf who delivers a disciplined , dig ##nified and highly sympathetic performance as a working class franco - irish kid fighting his way through the prejudice ##s of the new england wasp establishment . for those who are only familiar with his slap - stick performances in \" even stevens \" this demonstration of his maturity is a delightful surprise . and josh fl ##itte ##r as the ten year old cad ##dy threatens to steal every scene in which he appears . < br / > < br / > a old fashioned movie in the best sense of the word : fine acting , clear directing and a great story that grips to the end - the final scene an affection ##ate nod to casablanca is just one of the many pleasures that fill a great movie . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.336301 140467944662912 run_classifier.py:464] tokens: [CLS] bill paxton has taken the true story of the 1913 us golf open and made a film that is about much more than an extra - ordinary game of golf . the film also deals directly with the class tensions of the early twentieth century and touches upon the profound anti - catholic prejudice ##s of both the british and american establishments . but at heart the film is about that perennial favourite of triumph against the odds . < br / > < br / > the acting is exemplary throughout . stephen dil ##lane is excellent as usual , but the revelation of the movie is shia lab ##oe ##uf who delivers a disciplined , dig ##nified and highly sympathetic performance as a working class franco - irish kid fighting his way through the prejudice ##s of the new england wasp establishment . for those who are only familiar with his slap - stick performances in \" even stevens \" this demonstration of his maturity is a delightful surprise . and josh fl ##itte ##r as the ten year old cad ##dy threatens to steal every scene in which he appears . < br / > < br / > a old fashioned movie in the best sense of the word : fine acting , clear directing and a great story that grips to the end - the final scene an affection ##ate nod to casablanca is just one of the many pleasures that fill a great movie . [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3021 27765 2038 2579 1996 2995 2466 1997 1996 5124 2149 5439 2330 1998 2081 1037 2143 2008 2003 2055 2172 2062 2084 2019 4469 1011 6623 2208 1997 5439 1012 1996 2143 2036 9144 3495 2007 1996 2465 13136 1997 1996 2220 9086 2301 1998 12817 2588 1996 13769 3424 1011 3234 18024 2015 1997 2119 1996 2329 1998 2137 17228 1012 2021 2012 2540 1996 2143 2003 2055 2008 14638 8837 1997 10911 2114 1996 10238 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 3772 2003 27792 2802 1012 4459 29454 20644 2003 6581 2004 5156 1010 2021 1996 11449 1997 1996 3185 2003 20474 6845 8913 16093 2040 18058 1037 28675 1010 10667 25201 1998 3811 13026 2836 2004 1037 2551 2465 9341 1011 3493 4845 3554 2010 2126 2083 1996 18024 2015 1997 1996 2047 2563 19411 5069 1012 2005 2216 2040 2024 2069 5220 2007 2010 14308 1011 6293 4616 1999 1000 2130 8799 1000 2023 10467 1997 2010 16736 2003 1037 26380 4474 1012 1998 6498 13109 27100 2099 2004 1996 2702 2095 2214 28353 5149 17016 2000 8954 2296 3496 1999 2029 2002 3544 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 2214 13405 3185 1999 1996 2190 3168 1997 1996 2773 1024 2986 3772 1010 3154 9855 1998 1037 2307 2466 2008 24758 2000 1996 2203 1011 1996 2345 3496 2019 12242 3686 7293 2000 24592 2003 2074 2028 1997 1996 2116 26552 2008 6039 1037 2307 3185 1012 102 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.338325 140467944662912 run_classifier.py:465] input_ids: 101 3021 27765 2038 2579 1996 2995 2466 1997 1996 5124 2149 5439 2330 1998 2081 1037 2143 2008 2003 2055 2172 2062 2084 2019 4469 1011 6623 2208 1997 5439 1012 1996 2143 2036 9144 3495 2007 1996 2465 13136 1997 1996 2220 9086 2301 1998 12817 2588 1996 13769 3424 1011 3234 18024 2015 1997 2119 1996 2329 1998 2137 17228 1012 2021 2012 2540 1996 2143 2003 2055 2008 14638 8837 1997 10911 2114 1996 10238 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 3772 2003 27792 2802 1012 4459 29454 20644 2003 6581 2004 5156 1010 2021 1996 11449 1997 1996 3185 2003 20474 6845 8913 16093 2040 18058 1037 28675 1010 10667 25201 1998 3811 13026 2836 2004 1037 2551 2465 9341 1011 3493 4845 3554 2010 2126 2083 1996 18024 2015 1997 1996 2047 2563 19411 5069 1012 2005 2216 2040 2024 2069 5220 2007 2010 14308 1011 6293 4616 1999 1000 2130 8799 1000 2023 10467 1997 2010 16736 2003 1037 26380 4474 1012 1998 6498 13109 27100 2099 2004 1996 2702 2095 2214 28353 5149 17016 2000 8954 2296 3496 1999 2029 2002 3544 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 2214 13405 3185 1999 1996 2190 3168 1997 1996 2773 1024 2986 3772 1010 3154 9855 1998 1037 2307 2466 2008 24758 2000 1996 2203 1011 1996 2345 3496 2019 12242 3686 7293 2000 24592 2003 2074 2028 1997 1996 2116 26552 2008 6039 1037 2307 3185 1012 102 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.340798 140467944662912 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.343243 140467944662912 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:25:58.345222 140467944662912 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc0d287dc18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:26:46.024969 140467944662912 estimator.py:201] Using config: {'_model_dir': 'output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc0d287dc18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:27:18.614695 140467944662912 estimator.py:1111] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:27:22.271885 140467944662912 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:27:32.093136 140467944662912 estimator.py:1113] Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:27:32.096809 140467944662912 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:27:33.536356 140467944662912 monitored_session.py:222] Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:27:38.365977 140467944662912 session_manager.py:491] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:27:38.632759 140467944662912 session_manager.py:493] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into output/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:28:10.529274 140467944662912 basic_session_run_hooks.py:594] Saving checkpoints for 0 into output/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6993848, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:28:28.893009 140467944662912 basic_session_run_hooks.py:249] loss = 0.6993848, step = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.501551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:31:48.273236 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.501551\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.23517969, step = 100 (199.387 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:31:48.280215 140467944662912 basic_session_run_hooks.py:247] loss = 0.23517969, step = 100 (199.387 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.529904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:34:56.986574 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.529904\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.46643195, step = 200 (188.711 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:34:56.991413 140467944662912 basic_session_run_hooks.py:247] loss = 0.46643195, step = 200 (188.711 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:38:04.916179 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532114\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.12518743, step = 300 (187.930 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:38:04.921881 140467944662912 basic_session_run_hooks.py:247] loss = 0.12518743, step = 300 (187.930 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:41:13.395540 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530562\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.23391683, step = 400 (188.479 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:41:13.400693 140467944662912 basic_session_run_hooks.py:247] loss = 0.23391683, step = 400 (188.479 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:44:21.197503 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532476\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.21998064, step = 500 (187.804 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:44:21.205190 140467944662912 basic_session_run_hooks.py:247] loss = 0.21998064, step = 500 (187.804 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.531472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:47:29.354123 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.531472\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.13359639, step = 600 (188.151 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:47:29.356432 140467944662912 basic_session_run_hooks.py:247] loss = 0.13359639, step = 600 (188.151 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:50:37.853677 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530505\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.064320795, step = 700 (188.501 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:50:37.857908 140467944662912 basic_session_run_hooks.py:247] loss = 0.064320795, step = 700 (188.501 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:53:45.685051 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532392\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.12234235, step = 800 (187.831 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:53:45.688899 140467944662912 basic_session_run_hooks.py:247] loss = 0.12234235, step = 800 (187.831 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:56:53.443795 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532598\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.20830041, step = 900 (187.759 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 08:56:53.447860 140467944662912 basic_session_run_hooks.py:247] loss = 0.20830041, step = 900 (187.759 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.531399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:00:01.626250 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.531399\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.16599476, step = 1000 (188.183 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:00:01.631138 140467944662912 basic_session_run_hooks.py:247] loss = 0.16599476, step = 1000 (188.183 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.531603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:03:09.736449 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.531603\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.50197387, step = 1100 (188.111 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:03:09.741712 140467944662912 basic_session_run_hooks.py:247] loss = 0.50197387, step = 1100 (188.111 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:06:18.098015 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530894\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.2146865, step = 1200 (188.362 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:06:18.104138 140467944662912 basic_session_run_hooks.py:247] loss = 0.2146865, step = 1200 (188.362 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:09:25.840408 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532645\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.023117196, step = 1300 (187.739 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:09:25.843058 140467944662912 basic_session_run_hooks.py:247] loss = 0.023117196, step = 1300 (187.739 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:12:33.787599 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532065\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.015641412, step = 1400 (187.950 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:12:33.792591 140467944662912 basic_session_run_hooks.py:247] loss = 0.015641412, step = 1400 (187.950 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:15:41.724969 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532092\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.008165626, step = 1500 (187.935 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:15:41.727245 140467944662912 basic_session_run_hooks.py:247] loss = 0.008165626, step = 1500 (187.935 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.531701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:18:49.800604 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.531701\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.08675922, step = 1600 (188.077 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:18:49.804689 140467944662912 basic_session_run_hooks.py:247] loss = 0.08675922, step = 1600 (188.077 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:21:58.125378 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530998\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.00940958, step = 1700 (188.326 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:21:58.130448 140467944662912 basic_session_run_hooks.py:247] loss = 0.00940958, step = 1700 (188.326 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.531442\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:25:06.292784 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.531442\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.35310686, step = 1800 (188.166 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:25:06.296096 140467944662912 basic_session_run_hooks.py:247] loss = 0.35310686, step = 1800 (188.166 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:28:14.126793 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532385\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.0547651, step = 1900 (187.837 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:28:14.132698 140467944662912 basic_session_run_hooks.py:247] loss = 0.0547651, step = 1900 (187.837 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.529761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:31:22.891179 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.529761\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.34223968, step = 2000 (188.763 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:31:22.895314 140467944662912 basic_session_run_hooks.py:247] loss = 0.34223968, step = 2000 (188.763 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:34:30.808744 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532148\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.0045011537, step = 2100 (187.917 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:34:30.811974 140467944662912 basic_session_run_hooks.py:247] loss = 0.0045011537, step = 2100 (187.917 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:37:39.482841 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530014\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.05614534, step = 2200 (188.675 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:37:39.486621 140467944662912 basic_session_run_hooks.py:247] loss = 0.05614534, step = 2200 (188.675 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:40:47.231426 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532627\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.001983378, step = 2300 (187.749 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:40:47.235821 140467944662912 basic_session_run_hooks.py:247] loss = 0.001983378, step = 2300 (187.749 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:43:55.739532 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530481\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.112457715, step = 2400 (188.506 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:43:55.741656 140467944662912 basic_session_run_hooks.py:247] loss = 0.112457715, step = 2400 (188.506 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.529797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:47:04.491017 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.529797\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.003345988, step = 2500 (188.752 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:47:04.493228 140467944662912 basic_session_run_hooks.py:247] loss = 0.003345988, step = 2500 (188.752 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.532229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:50:12.379945 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.532229\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.0040395088, step = 2600 (187.891 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:50:12.384561 140467944662912 basic_session_run_hooks.py:247] loss = 0.0040395088, step = 2600 (187.891 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.529844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:53:21.114773 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.529844\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.0052446094, step = 2700 (188.733 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:53:21.117072 140467944662912 basic_session_run_hooks.py:247] loss = 0.0052446094, step = 2700 (188.733 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:56:29.458620 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530944\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.0024259093, step = 2800 (188.352 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:56:29.469316 140467944662912 basic_session_run_hooks.py:247] loss = 0.0024259093, step = 2800 (188.352 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.531221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:59:37.704028 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.531221\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.050826464, step = 2900 (188.242 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 09:59:37.711686 140467944662912 basic_session_run_hooks.py:247] loss = 0.050826464, step = 2900 (188.242 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.530943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:02:46.048283 140467944662912 basic_session_run_hooks.py:680] global_step/sec: 0.530943\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.002394785, step = 3000 (188.340 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:02:46.051877 140467944662912 basic_session_run_hooks.py:247] loss = 0.002394785, step = 3000 (188.340 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 3046 into output/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:10.479188 140467944662912 basic_session_run_hooks.py:594] Saving checkpoints for 3046 into output/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.0025445516.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:21.478767 140467944662912 estimator.py:359] Loss for final step: 0.0025445516.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:31.791005 140467944662912 estimator.py:1111] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:35.279669 140467944662912 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:45.376442 140467944662912 estimator.py:1113] Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-05-30T10:04:45Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:45.407343 140467944662912 evaluation.py:257] Starting evaluation at 2019-05-30T10:04:45Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:47.734559 140467944662912 monitored_session.py:222] Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from output/model.ckpt-3046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:47.748889 140467944662912 saver.py:1270] Restoring parameters from output/model.ckpt-3046\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:50.155832 140467944662912 session_manager.py:491] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:04:50.411526 140467944662912 session_manager.py:493] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-05-30-10:08:23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:08:23.825225 140467944662912 evaluation.py:277] Finished evaluation at 2019-05-30-10:08:23\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 3046: auc = 0.9248, eval_accuracy = 0.9248, f1_score = 0.9227132, false_negatives = 511.0, false_positives = 241.0, global_step = 3046, loss = 0.3190446, precision = 0.94904864, recall = 0.8978, true_negatives = 4759.0, true_positives = 4489.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:08:23.827386 140467944662912 estimator.py:1979] Saving dict for global step 3046: auc = 0.9248, eval_accuracy = 0.9248, f1_score = 0.9227132, false_negatives = 511.0, false_positives = 241.0, global_step = 3046, loss = 0.3190446, precision = 0.94904864, recall = 0.8978, true_negatives = 4759.0, true_positives = 4489.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3046: output/model.ckpt-3046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0530 10:08:23.835960 140467944662912 estimator.py:2039] Saving 'checkpoint_path' summary for global step 3046: output/model.ckpt-3046\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLOvtveTMqwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "6494fe9e-434b-4ecf-c68c-cf876fa8d23c"
      },
      "source": [
        "# This is the result with learning rate of 2e-5\n",
        "pretty_print(result)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>auc</th>\n",
              "      <td>0.924800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eval_accuracy</th>\n",
              "      <td>0.924800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score</th>\n",
              "      <td>0.922713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>false_negatives</th>\n",
              "      <td>511.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>false_positives</th>\n",
              "      <td>241.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>global_step</th>\n",
              "      <td>3046.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loss</th>\n",
              "      <td>0.319045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.949049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.897800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_negatives</th>\n",
              "      <td>4759.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true_positives</th>\n",
              "      <td>4489.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      values\n",
              "auc                 0.924800\n",
              "eval_accuracy       0.924800\n",
              "f1_score            0.922713\n",
              "false_negatives   511.000000\n",
              "false_positives   241.000000\n",
              "global_step      3046.000000\n",
              "loss                0.319045\n",
              "precision           0.949049\n",
              "recall              0.897800\n",
              "true_negatives   4759.000000\n",
              "true_positives   4489.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0OPTDcGMsJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the result with learning rate of 1e-4\n",
        "pretty_print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}